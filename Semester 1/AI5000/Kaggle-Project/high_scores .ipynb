{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "Only the below code has given public scoreboard score as 0.60069  and private score 0.69376\n",
        "\n",
        "Link: https://www.kaggle.com/competitions/iith-foml-2023/overview\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "Jr7K-BtqR59r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "from scipy.stats import randint\n",
        "from sklearn.metrics import make_scorer, f1_score\n",
        "\n",
        "# Load the training data\n",
        "train_data = pd.read_csv('/content/drive/MyDrive/Kaggle_Competition_FOML/iith-foml-2023/iith_foml_2023_train.csv')\n",
        "\n",
        "X_train = train_data.drop('Target Variable (Discrete)', axis=1)\n",
        "y_train = train_data['Target Variable (Discrete)']\n",
        "\n",
        "# Load the test data\n",
        "test_data = pd.read_csv('/content/drive/MyDrive/Kaggle_Competition_FOML/iith-foml-2023/iith_foml_2023_test.csv')\n",
        "\n",
        "# Impute missing values\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "X_train_imputed = imputer.fit_transform(X_train)\n",
        "test_data_imputed = imputer.transform(test_data)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train_imputed)\n",
        "test_data_scaled = scaler.transform(test_data_imputed)\n",
        "\n",
        "# Define the model\n",
        "model = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Fine-tune hyperparameters based on the best hyperparameters obtained\n",
        "param_dist = {\n",
        "    'classifier__n_estimators': randint(400, 600),\n",
        "    'classifier__max_depth': [None] + list(randint(30, 40).rvs(10)),\n",
        "    'classifier__min_samples_split': randint(2, 5),\n",
        "    'feature_selector__threshold': [\"mean\", \"median\", \"1.25*mean\"]\n",
        "}\n",
        "\n",
        "# Define the feature selector\n",
        "feature_selector = SelectFromModel(RandomForestClassifier(n_estimators=100, random_state=42))\n",
        "\n",
        "# Create a pipeline\n",
        "pipeline = Pipeline([\n",
        "    ('imputer', imputer),\n",
        "    ('scaler', scaler),\n",
        "    ('feature_selector', feature_selector),\n",
        "    ('classifier', model)\n",
        "])\n",
        "\n",
        "# Define the F1 scorer for RandomizedSearchCV\n",
        "f1_scorer = make_scorer(f1_score, average='macro')\n",
        "\n",
        "# Perform RandomizedSearchCV with more iterations\n",
        "random_search = RandomizedSearchCV(pipeline, param_distributions=param_dist, cv=5, scoring=f1_scorer, n_iter=200)\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "# Print the best hyperparameters and corresponding F1 score\n",
        "print(\"Best Hyperparameters:\", random_search.best_params_)\n",
        "print(\"Best Macro F1 Score:\", random_search.best_score_)\n",
        "\n",
        "# Get the best model\n",
        "best_model = random_search.best_estimator_\n",
        "\n",
        "# Make predictions on the test set\n",
        "predicted_values = best_model.predict(test_data)\n",
        "\n",
        "# Save the submission file with default index as 'Id'\n",
        "submission_df = pd.DataFrame({\n",
        "    'Id': test_data.index + 1,  # Adding 1 to start index from 1\n",
        "    'Category': predicted_values\n",
        "})\n",
        "\n",
        "# Save the submission file\n",
        "submission_df.to_csv('submission_75.csv', index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dm60VJr1LmBm",
        "outputId": "55f737e2-34c0-4da2-bd75-cd5b9eac3760"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_split.py:700: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Hyperparameters: {'classifier__max_depth': 37, 'classifier__min_samples_split': 2, 'classifier__n_estimators': 410, 'feature_selector__threshold': 'median'}\n",
            "Best Macro F1 Score: 0.45383994605625555\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HhoRKSD-RdMQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GjlYQi_rRdAz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "Different Approaches\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "WYIndIYPRW71"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "Highest\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "s_k20Jf78bwp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "from scipy.stats import randint\n",
        "from sklearn.metrics import make_scorer, f1_score\n",
        "\n",
        "# Load the training data\n",
        "train_data = pd.read_csv('/content/drive/MyDrive/Kaggle_Competition_FOML/iith-foml-2023/iith_foml_2023_train.csv')\n",
        "\n",
        "X_train = train_data.drop('Target Variable (Discrete)', axis=1)\n",
        "y_train = train_data['Target Variable (Discrete)']\n",
        "\n",
        "# Load the test data\n",
        "test_data = pd.read_csv('/content/drive/MyDrive/Kaggle_Competition_FOML/iith-foml-2023/iith_foml_2023_test.csv')\n",
        "\n",
        "# Impute missing values\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "X_train_imputed = imputer.fit_transform(X_train)\n",
        "test_data_imputed = imputer.transform(test_data)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train_imputed)\n",
        "test_data_scaled = scaler.transform(test_data_imputed)\n",
        "\n",
        "# Define the model\n",
        "model = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Fine-tune hyperparameters based on the best hyperparameters obtained\n",
        "param_dist = {\n",
        "    'classifier__n_estimators': randint(300, 800),\n",
        "    'classifier__max_depth': [None] + list(randint(20, 50).rvs(10)),\n",
        "    'classifier__min_samples_split': randint(2, 10),\n",
        "    'feature_selector__threshold': [\"mean\", \"median\", \"1.25*mean\"]\n",
        "}\n",
        "\n",
        "# Define the feature selector\n",
        "feature_selector = SelectFromModel(RandomForestClassifier(n_estimators=100, random_state=42))\n",
        "\n",
        "# Create a pipeline\n",
        "pipeline = Pipeline([\n",
        "    ('imputer', imputer),\n",
        "    ('scaler', scaler),\n",
        "    ('feature_selector', feature_selector),\n",
        "    ('classifier', model)\n",
        "])\n",
        "\n",
        "# Define the F1 scorer for RandomizedSearchCV\n",
        "f1_scorer = make_scorer(f1_score, average='macro')\n",
        "\n",
        "# Perform RandomizedSearchCV with more iterations\n",
        "random_search = RandomizedSearchCV(pipeline, param_distributions=param_dist, cv=5, scoring=f1_scorer, n_iter=500)\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "# Print the best hyperparameters and corresponding F1 score\n",
        "print(\"Best Hyperparameters:\", random_search.best_params_)\n",
        "print(\"Best Macro F1 Score:\", random_search.best_score_)\n",
        "\n",
        "# Get the best model\n",
        "best_model = random_search.best_estimator_\n",
        "\n",
        "# Make predictions on the test set\n",
        "predicted_values = best_model.predict(test_data)\n",
        "\n",
        "# Save the submission file with default index as 'Id'\n",
        "submission_df = pd.DataFrame({\n",
        "    'Id': test_data.index + 1,  # Adding 1 to start index from 1\n",
        "    'Category': predicted_values\n",
        "})\n",
        "\n",
        "# Save the submission file\n",
        "submission_df.to_csv('submission_updated.csv', index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2rEWQllqU2wQ",
        "outputId": "384ec718-db19-4c09-8ccb-6a3bd922a409"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_split.py:700: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Hyperparameters: {'classifier__max_depth': 45, 'classifier__min_samples_split': 2, 'classifier__n_estimators': 353, 'feature_selector__threshold': 'median'}\n",
            "Best Macro F1 Score: 0.4540905374436514\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qP6hY0zVyA9z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "idk\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "_JGkClJO363W"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JhlJEwkAyA1B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "Testing\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "ZstCK9hyyB1N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "from sklearn.metrics import make_scorer, f1_score\n",
        "\n",
        "# Load the training data\n",
        "train_data = pd.read_csv('/content/drive/MyDrive/Kaggle_Competition_FOML/iith-foml-2023/iith_foml_2023_train.csv')\n",
        "\n",
        "X_train = train_data.drop('Target Variable (Discrete)', axis=1)\n",
        "y_train = train_data['Target Variable (Discrete)']\n",
        "\n",
        "# Load the test data\n",
        "test_data = pd.read_csv('/content/drive/MyDrive/Kaggle_Competition_FOML/iith-foml-2023/iith_foml_2023_test.csv')\n",
        "\n",
        "# Impute missing values\n",
        "imputer = SimpleImputer(strategy='median')\n",
        "X_train_imputed = imputer.fit_transform(X_train)\n",
        "test_data_imputed = imputer.transform(test_data)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train_imputed)\n",
        "test_data_scaled = scaler.transform(test_data_imputed)\n",
        "\n",
        "# Define the model\n",
        "model = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Fixed hyperparameter values\n",
        "param_dist = {\n",
        "    'classifier__n_estimators': [413],\n",
        "    'classifier__max_depth': [35],\n",
        "    'classifier__min_samples_split': [2],\n",
        "    # 'feature_selector__threshold': [\"0.35*median\"]\n",
        "    'feature_selector__threshold': [\"0.35*median\"]\n",
        "}\n",
        "\n",
        "# Define the feature selector\n",
        "feature_selector = SelectFromModel(RandomForestClassifier(n_estimators=100, random_state=42))\n",
        "\n",
        "# Create a pipeline\n",
        "pipeline = Pipeline([\n",
        "    ('imputer', imputer),\n",
        "    ('scaler', scaler),\n",
        "    ('feature_selector', feature_selector),\n",
        "    ('classifier', model)\n",
        "])\n",
        "\n",
        "# Define the F1 scorer for RandomizedSearchCV\n",
        "f1_scorer = make_scorer(f1_score, average='macro')\n",
        "\n",
        "# Perform RandomizedSearchCV with more iterations\n",
        "random_search = RandomizedSearchCV(pipeline, param_distributions=[param_dist], cv=5, scoring=f1_scorer, n_iter=500)\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "# Print the best hyperparameters and corresponding F1 score\n",
        "print(\"Best Hyperparameters:\", random_search.best_params_)\n",
        "print(\"Best Macro F1 Score:\", random_search.best_score_)\n",
        "\n",
        "# Get the best model\n",
        "best_model = random_search.best_estimator_\n",
        "\n",
        "# Make predictions on the test set\n",
        "predicted_values = best_model.predict(test_data)\n",
        "\n",
        "# Save the submission file with default index as 'Id'\n",
        "submission_df = pd.DataFrame({\n",
        "    'Id': test_data.index + 1,  # Adding 1 to start index from 1\n",
        "    'Category': predicted_values\n",
        "})\n",
        "\n",
        "# Save the submission file\n",
        "submission_df.to_csv('submission_mean_extended.csv', index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fc7gcSwrx7-a",
        "outputId": "e8bf5519-42c4-47ef-a040-c4548b4d3d5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py:305: UserWarning: The total space of parameters 1 is smaller than n_iter=500. Running 1 iterations. For exhaustive searches, use GridSearchCV.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_split.py:700: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Hyperparameters: {'feature_selector__threshold': '0.35*median', 'classifier__n_estimators': 413, 'classifier__min_samples_split': 2, 'classifier__max_depth': 35}\n",
            "Best Macro F1 Score: 0.46131981181264753\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "from sklearn.metrics import make_scorer, f1_score\n",
        "\n",
        "# Load the training data\n",
        "train_data = pd.read_csv('/content/drive/MyDrive/Kaggle_Competition_FOML/iith-foml-2023/iith_foml_2023_train.csv')\n",
        "\n",
        "X_train = train_data.drop('Target Variable (Discrete)', axis=1)\n",
        "y_train = train_data['Target Variable (Discrete)']\n",
        "\n",
        "# Load the test data\n",
        "test_data = pd.read_csv('/content/drive/MyDrive/Kaggle_Competition_FOML/iith-foml-2023/iith_foml_2023_test.csv')\n",
        "\n",
        "# Impute missing values\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "X_train_imputed = imputer.fit_transform(X_train)\n",
        "test_data_imputed = imputer.transform(test_data)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train_imputed)\n",
        "test_data_scaled = scaler.transform(test_data_imputed)\n",
        "\n",
        "# Define the model\n",
        "model = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Fixed hyperparameter values\n",
        "param_dist = {\n",
        "    'classifier__n_estimators': [353],\n",
        "    'classifier__max_depth': [35],\n",
        "    'classifier__min_samples_split': [2],\n",
        "    # 'feature_selector__threshold': [\"0.35*median\"]\n",
        "    'feature_selector__threshold': [\"0.35*median\"]\n",
        "}\n",
        "\n",
        "# Define the feature selector\n",
        "feature_selector = SelectFromModel(RandomForestClassifier(n_estimators=100, random_state=42))\n",
        "\n",
        "# Create a pipeline\n",
        "pipeline = Pipeline([\n",
        "    ('imputer', imputer),\n",
        "    ('scaler', scaler),\n",
        "    ('feature_selector', feature_selector),\n",
        "    ('classifier', model)\n",
        "])\n",
        "\n",
        "# Define the F1 scorer for RandomizedSearchCV\n",
        "f1_scorer = make_scorer(f1_score, average='macro')\n",
        "\n",
        "# Perform RandomizedSearchCV with more iterations\n",
        "random_search = RandomizedSearchCV(pipeline, param_distributions=[param_dist], cv=50, scoring=f1_scorer, n_iter=1500)\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "# Print the best hyperparameters and corresponding F1 score\n",
        "print(\"Best Hyperparameters:\", random_search.best_params_)\n",
        "print(\"Best Macro F1 Score:\", random_search.best_score_)\n",
        "\n",
        "# Get the best model\n",
        "best_model = random_search.best_estimator_\n",
        "\n",
        "# Make predictions on the test set\n",
        "predicted_values = best_model.predict(test_data)\n",
        "\n",
        "# Save the submission file with default index as 'Id'\n",
        "submission_df = pd.DataFrame({\n",
        "    'Id': test_data.index + 1,  # Adding 1 to start index from 1\n",
        "    'Category': predicted_values\n",
        "})\n",
        "\n",
        "# Save the submission file\n",
        "submission_df.to_csv('submission_hjj.csv', index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rU8BF6WJ2NLG",
        "outputId": "a13ed090-52fc-4165-8b28-b680429a915f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py:305: UserWarning: The total space of parameters 1 is smaller than n_iter=1500. Running 1 iterations. For exhaustive searches, use GridSearchCV.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_split.py:700: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=50.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Hyperparameters: {'feature_selector__threshold': '0.35*median', 'classifier__n_estimators': 353, 'classifier__min_samples_split': 2, 'classifier__max_depth': 35}\n",
            "Best Macro F1 Score: 0.773831807981034\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "j2MugtVoKzKb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the training data\n",
        "train_data = pd.read_csv('/content/drive/MyDrive/Kaggle_Competition_FOML/iith-foml-2023/iith_foml_2023_train.csv')\n",
        "\n",
        "X_train = train_data.drop('Target Variable (Discrete)', axis=1)\n",
        "y_train = train_data['Target Variable (Discrete)']\n",
        "\n",
        "# Load the test data\n",
        "test_data = pd.read_csv('/content/drive/MyDrive/Kaggle_Competition_FOML/iith-foml-2023/iith_foml_2023_test.csv')\n",
        "\n",
        "# Feature Engineering\n",
        "def feature_engineering(data):\n",
        "    # Interaction Terms\n",
        "    data['Interaction_1_2'] = data['Feature 1 (Discrete)'] * data['Feature 2 (Discrete)']\n",
        "\n",
        "    # Polynomial Features\n",
        "    poly = PolynomialFeatures(degree=2, include_bias=False)\n",
        "    poly_features = poly.fit_transform(data[['Feature 1 (Discrete)', 'Feature 2 (Discrete)']])\n",
        "    poly_df = pd.DataFrame(poly_features, columns=[f'Poly_{i}' for i in range(poly_features.shape[1])])\n",
        "    data = pd.concat([data, poly_df], axis=1)\n",
        "\n",
        "    # Aggregations\n",
        "    agg_features = data.groupby('Feature 19 (Discrete)')['Feature 9'].agg(['mean', 'max', 'min']).reset_index()\n",
        "    agg_features.columns = ['Feature 19 (Discrete)', 'Agg_mean', 'Agg_max', 'Agg_min']\n",
        "    data = pd.merge(data, agg_features, on='Feature 19 (Discrete)', how='left')\n",
        "\n",
        "    return data\n",
        "\n",
        "# Apply feature engineering\n",
        "X_train = feature_engineering(X_train)\n",
        "test_data = feature_engineering(test_data)\n",
        "\n",
        "# Impute missing values\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "X_train_imputed = imputer.fit_transform(X_train)\n",
        "test_data_imputed = imputer.transform(test_data)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train_imputed)\n",
        "test_data_scaled = scaler.transform(test_data_imputed)\n",
        "\n",
        "# Define the model\n",
        "model = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Fixed hyperparameter values\n",
        "param_dist = {\n",
        "    'classifier__n_estimators': [353],\n",
        "    'classifier__max_depth': [35],\n",
        "    'classifier__min_samples_split': [2],\n",
        "    'feature_selector__threshold': [\"0.35*median\"]\n",
        "}\n",
        "\n",
        "# Define the feature selector\n",
        "feature_selector = SelectFromModel(RandomForestClassifier(n_estimators=100, random_state=42))\n",
        "\n",
        "# Create a pipeline\n",
        "pipeline = Pipeline([\n",
        "    ('imputer', imputer),\n",
        "    ('scaler', scaler),\n",
        "    ('feature_selector', feature_selector),\n",
        "    ('classifier', model)\n",
        "])\n",
        "\n",
        "# Define the F1 scorer for RandomizedSearchCV\n",
        "f1_scorer = make_scorer(f1_score, average='macro')\n",
        "\n",
        "# Perform RandomizedSearchCV with more iterations\n",
        "random_search = RandomizedSearchCV(pipeline, param_distributions=[param_dist], cv=50, scoring=f1_scorer, n_iter=1500)\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "# Print the best hyperparameters and corresponding F1 score\n",
        "print(\"Best Hyperparameters:\", random_search.best_params_)\n",
        "print(\"Best Macro F1 Score:\", random_search.best_score_)\n",
        "\n",
        "# Get the best model\n",
        "best_model = random_search.best_estimator_\n",
        "\n",
        "# Make predictions on the test set\n",
        "predicted_values = best_model.predict(test_data)\n",
        "\n",
        "# Save the submission file with default index as 'Id'\n",
        "submission_df = pd.DataFrame({\n",
        "    'Id': test_data.index + 1,  # Adding 1 to start index from 1\n",
        "    'Category': predicted_values\n",
        "})\n",
        "\n",
        "# Save the submission file\n",
        "submission_df.to_csv('submission_special.csv', index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HskCDfO5KzC0",
        "outputId": "a464c89d-cb17-40bd-f4af-73ffabc6ca71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py:305: UserWarning: The total space of parameters 1 is smaller than n_iter=1500. Running 1 iterations. For exhaustive searches, use GridSearchCV.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_split.py:700: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=50.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Hyperparameters: {'feature_selector__threshold': '0.35*median', 'classifier__n_estimators': 353, 'classifier__min_samples_split': 2, 'classifier__max_depth': 35}\n",
            "Best Macro F1 Score: 0.7747828206880838\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6E6yMaxVKy_3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import RandomizedSearchCV, train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "from sklearn.metrics import make_scorer, f1_score\n",
        "\n",
        "# Load the training data\n",
        "train_data = pd.read_csv('/content/drive/MyDrive/Kaggle_Competition_FOML/iith-foml-2023/iith_foml_2023_train.csv')\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    train_data.drop('Target Variable (Discrete)', axis=1),\n",
        "    train_data['Target Variable (Discrete)'],\n",
        "    test_size=0.2,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Load the test data\n",
        "test_data = pd.read_csv('/content/drive/MyDrive/Kaggle_Competition_FOML/iith-foml-2023/iith_foml_2023_test.csv')\n",
        "\n",
        "# Combine datasets for consistent feature engineering\n",
        "combined_data = pd.concat([X_train, X_val, test_data])\n",
        "\n",
        "# Reset index\n",
        "combined_data.reset_index(drop=True, inplace=True)\n",
        "\n",
        "# Feature Engineering\n",
        "def feature_engineering(data):\n",
        "    # Interaction Terms\n",
        "    data['Interaction_1_2'] = data['Feature 1 (Discrete)'] * data['Feature 2 (Discrete)']\n",
        "\n",
        "    # Polynomial Features\n",
        "    poly = PolynomialFeatures(degree=2, include_bias=False)\n",
        "    poly_features = poly.fit_transform(data[['Feature 1 (Discrete)', 'Feature 2 (Discrete)']])\n",
        "    poly_df = pd.DataFrame(poly_features, columns=[f'Poly_{i}' for i in range(poly_features.shape[1])])\n",
        "    data = pd.concat([data, poly_df], axis=1)\n",
        "\n",
        "    # Aggregations\n",
        "    agg_features = data.groupby('Feature 19 (Discrete)')['Feature 9'].agg(['mean', 'max', 'min']).reset_index()\n",
        "    agg_features.columns = ['Feature 19 (Discrete)', 'Agg_mean', 'Agg_max', 'Agg_min']\n",
        "    data = pd.merge(data, agg_features, on='Feature 19 (Discrete)', how='left')\n",
        "\n",
        "    return data\n",
        "\n",
        "# Apply feature engineering\n",
        "combined_data = feature_engineering(combined_data)\n",
        "\n",
        "# Split the datasets back into training and test sets\n",
        "X_train = combined_data.iloc[:len(X_train), :]\n",
        "X_val = combined_data.iloc[len(X_train):(len(X_train) + len(X_val)), :]\n",
        "test_data = combined_data.iloc[(len(X_train) + len(X_val)):, :]\n",
        "\n",
        "# Impute missing values\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "X_train_imputed = imputer.fit_transform(X_train)\n",
        "X_val_imputed = imputer.transform(X_val)\n",
        "test_data_imputed = imputer.transform(test_data)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train_imputed)\n",
        "X_val_scaled = scaler.transform(X_val_imputed)\n",
        "test_data_scaled = scaler.transform(test_data_imputed)\n",
        "\n",
        "# Define the model\n",
        "model = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Fixed hyperparameter values\n",
        "param_dist = {\n",
        "    'n_estimators': [353],\n",
        "    'max_depth': [35],\n",
        "    'min_samples_split': [2],\n",
        "    'feature_selector__threshold': [\"0.35*median\"]\n",
        "}\n",
        "\n",
        "# Define the feature selector\n",
        "feature_selector = SelectFromModel(RandomForestClassifier(n_estimators=100, random_state=42))\n",
        "\n",
        "# Create a pipeline\n",
        "pipeline = Pipeline([\n",
        "    ('imputer', imputer),\n",
        "    ('scaler', scaler),\n",
        "    ('feature_selector', feature_selector),\n",
        "    ('classifier', model)\n",
        "])\n",
        "\n",
        "# Define the F1 scorer for RandomizedSearchCV\n",
        "f1_scorer = make_scorer(f1_score, average='macro')\n",
        "\n",
        "# # Perform RandomizedSearchCV with more iterations\n",
        "# random_search = RandomizedSearchCV(pipeline, param_distributions=[param_dist], cv=5, scoring=f1_scorer, n_iter=1500)\n",
        "# random_search.fit(X_train_scaled, y_train)\n",
        "# Fixed hyperparameter values\n",
        "param_dist = {\n",
        "    'classifier__n_estimators': [353],\n",
        "    'classifier__max_depth': [35],\n",
        "    'classifier__min_samples_split': [2],\n",
        "    'feature_selector__threshold': [\"0.35*median\"]\n",
        "}\n",
        "\n",
        "# Perform RandomizedSearchCV with more iterations\n",
        "random_search = RandomizedSearchCV(pipeline, param_distributions=[param_dist], cv=50, scoring=f1_scorer, n_iter=1500)\n",
        "random_search.fit(X_train_scaled, y_train)\n",
        "\n",
        "\n",
        "# Print the best hyperparameters and corresponding F1 score\n",
        "print(\"Best Hyperparameters:\", random_search.best_params_)\n",
        "print(\"Best Macro F1 Score on Validation Set:\", random_search.best_score_)\n",
        "\n",
        "# Get the best model\n",
        "best_model = random_search.best_estimator_\n",
        "\n",
        "# Make predictions on the test set\n",
        "predicted_values = best_model.predict(test_data_scaled)\n",
        "\n",
        "# Save the submission file with default index as 'Id'\n",
        "submission_df = pd.DataFrame({\n",
        "    'Id': test_data.index + 1,  # Adding 1 to start index from 1\n",
        "    'Category': predicted_values\n",
        "})\n",
        "\n",
        "# Save the submission file\n",
        "submission_df.to_csv('submission_try.csv', index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "76r1m50VE6TX",
        "outputId": "45363c54-4b2f-4a7d-fc78-a7cfe8b4b02c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py:305: UserWarning: The total space of parameters 1 is smaller than n_iter=1500. Running 1 iterations. For exhaustive searches, use GridSearchCV.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_split.py:700: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=50.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Hyperparameters: {'feature_selector__threshold': '0.35*median', 'classifier__n_estimators': 353, 'classifier__min_samples_split': 2, 'classifier__max_depth': 35}\n",
            "Best Macro F1 Score on Validation Set: 0.7836764035528742\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Kj6WQACXgopq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "from sklearn.metrics import make_scorer, f1_score\n",
        "\n",
        "# Load the training data\n",
        "train_data = pd.read_csv('/content/drive/MyDrive/Kaggle_Competition_FOML/iith-foml-2023/iith_foml_2023_train.csv')\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    train_data.drop('Target Variable (Discrete)', axis=1),\n",
        "    train_data['Target Variable (Discrete)'],\n",
        "    test_size=0.2,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Combine datasets for consistent feature engineering\n",
        "combined_data = pd.concat([X_train, X_val])\n",
        "\n",
        "# Reset index\n",
        "combined_data.reset_index(drop=True, inplace=True)\n",
        "\n",
        "# Feature Engineering\n",
        "def feature_engineering(data):\n",
        "    # Interaction Terms\n",
        "    data['Interaction_1_2'] = data['Feature 1 (Discrete)'] * data['Feature 2 (Discrete)']\n",
        "\n",
        "    # Polynomial Features\n",
        "    poly = PolynomialFeatures(degree=2, include_bias=False)\n",
        "    poly_features = poly.fit_transform(data[['Feature 1 (Discrete)', 'Feature 2 (Discrete)']])\n",
        "    poly_df = pd.DataFrame(poly_features, columns=[f'Poly_{i}' for i in range(poly_features.shape[1])])\n",
        "    data = pd.concat([data, poly_df], axis=1)\n",
        "\n",
        "    # Aggregations\n",
        "    agg_features = data.groupby('Feature 19 (Discrete)')['Feature 9'].agg(['mean', 'max', 'min']).reset_index()\n",
        "    agg_features.columns = ['Feature 19 (Discrete)', 'Agg_mean', 'Agg_max', 'Agg_min']\n",
        "    data = pd.merge(data, agg_features, on='Feature 19 (Discrete)', how='left')\n",
        "\n",
        "    return data\n",
        "\n",
        "# Apply feature engineering\n",
        "combined_data = feature_engineering(combined_data)\n",
        "\n",
        "# Split the datasets back into training and validation sets\n",
        "X_train = combined_data.iloc[:len(X_train), :]\n",
        "X_val = combined_data.iloc[len(X_train):, :]\n",
        "\n",
        "# Impute missing values\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "X_train_imputed = imputer.fit_transform(X_train)\n",
        "X_val_imputed = imputer.transform(X_val)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train_imputed)\n",
        "X_val_scaled = scaler.transform(X_val_imputed)\n",
        "\n",
        "# Define the model\n",
        "model = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Define the feature selector\n",
        "feature_selector = SelectFromModel(RandomForestClassifier(n_estimators=100, random_state=42))\n",
        "\n",
        "# Create a pipeline\n",
        "pipeline = Pipeline([\n",
        "    ('imputer', imputer),\n",
        "    ('scaler', scaler),\n",
        "    ('feature_selector', feature_selector),\n",
        "    ('classifier', model)\n",
        "])\n",
        "\n",
        "# Define the F1 scorer for GridSearchCV\n",
        "f1_scorer = make_scorer(f1_score, average='macro')\n",
        "\n",
        "# Hyperparameter grid for GridSearchCV\n",
        "param_grid = {\n",
        "    'classifier__n_estimators': [100, 200, 300],\n",
        "    'classifier__max_depth': [10, 20, 30, None],\n",
        "    'classifier__min_samples_split': [2, 5, 10],\n",
        "    'feature_selector__threshold': [\"0.35*median\"]\n",
        "}\n",
        "\n",
        "# Perform GridSearchCV with more thorough hyperparameter search\n",
        "grid_search = GridSearchCV(pipeline, param_grid=param_grid, cv=20, scoring=f1_scorer)\n",
        "grid_search.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Print the best hyperparameters and corresponding F1 score\n",
        "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
        "print(\"Best Macro F1 Score on Validation Set:\", grid_search.best_score_)\n",
        "\n",
        "# Get the best model\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Make predictions on the test set\n",
        "predicted_values = best_model.predict(X_val_scaled)\n",
        "\n",
        "# Save the submission file with default index as 'Id'\n",
        "submission_df = pd.DataFrame({\n",
        "    'Id': X_val.index + 1,  # Adding 1 to start index from 1\n",
        "    'Category': predicted_values\n",
        "})\n",
        "\n",
        "# Save the submission file\n",
        "submission_df.to_csv('submission_try_2.csv', index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ff8Za6APbEAB",
        "outputId": "0c49e956-1373-4bff-d5fe-46d06e91b7ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_split.py:700: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=20.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Hyperparameters: {'classifier__max_depth': 20, 'classifier__min_samples_split': 2, 'classifier__n_estimators': 100, 'feature_selector__threshold': '0.35*median'}\n",
            "Best Macro F1 Score on Validation Set: 0.7173028730325173\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WJ8vJ4iagb5S"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}